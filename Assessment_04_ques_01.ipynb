{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assessment_04_ques_01.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMhK1ZxXgUr3SSI3GVcxQfD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sheetalm21/deep_learning/blob/assignment_4/Assessment_04_ques_01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4QhSaE7GR2be"
      },
      "source": [
        "### Sheetal Mahajan_20MAI0066"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZJZvOlhSEff"
      },
      "source": [
        "### Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IxXn_SBsRv3-"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ineUGnZ_SAFd"
      },
      "source": [
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Dropout, Flatten, Conv2D, MaxPooling2D\n",
        "from keras.layers.normalization import BatchNormalization"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FVHMCEpqSU-D"
      },
      "source": [
        "### AlexNet Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_7jAh9TjSDlf",
        "outputId": "8139ef9f-69b4-4522-9dea-bef324217815"
      },
      "source": [
        "AlexNet = Sequential()\n",
        "\n",
        "#1st Convolutional Layer\n",
        "AlexNet.add(Conv2D(filters=96, input_shape=(32,32,3), kernel_size=(11,11), strides=(4,4), padding='same'))\n",
        "AlexNet.add(BatchNormalization())\n",
        "AlexNet.add(Activation('relu'))\n",
        "AlexNet.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same'))\n",
        "\n",
        "#2nd Convolutional Layer\n",
        "AlexNet.add(Conv2D(filters=256, kernel_size=(5, 5), strides=(1,1), padding='same'))\n",
        "AlexNet.add(BatchNormalization())\n",
        "AlexNet.add(Activation('relu'))\n",
        "AlexNet.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same'))\n",
        "\n",
        "#3rd Convolutional Layer\n",
        "AlexNet.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='same'))\n",
        "AlexNet.add(BatchNormalization())\n",
        "AlexNet.add(Activation('relu'))\n",
        "\n",
        "#4th Convolutional Layer\n",
        "AlexNet.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='same'))\n",
        "AlexNet.add(BatchNormalization())\n",
        "AlexNet.add(Activation('relu'))\n",
        "\n",
        "#5th Convolutional Layer\n",
        "AlexNet.add(Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), padding='same'))\n",
        "AlexNet.add(BatchNormalization())\n",
        "AlexNet.add(Activation('relu'))\n",
        "AlexNet.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same'))\n",
        "\n",
        "#Passing it to a Fully Connected layer\n",
        "AlexNet.add(Flatten())\n",
        "# 1st Fully Connected Layer\n",
        "AlexNet.add(Dense(4096, input_shape=(32,32,3,)))\n",
        "AlexNet.add(BatchNormalization())\n",
        "AlexNet.add(Activation('relu'))\n",
        "# Add Dropout to prevent overfitting\n",
        "AlexNet.add(Dropout(0.4))\n",
        "\n",
        "#2nd Fully Connected Layer\n",
        "AlexNet.add(Dense(4096))\n",
        "AlexNet.add(BatchNormalization())\n",
        "AlexNet.add(Activation('relu'))\n",
        "#Add Dropout\n",
        "AlexNet.add(Dropout(0.4))\n",
        "\n",
        "#3rd Fully Connected Layer\n",
        "AlexNet.add(Dense(1000))\n",
        "AlexNet.add(BatchNormalization())\n",
        "AlexNet.add(Activation('relu'))\n",
        "#Add Dropout\n",
        "AlexNet.add(Dropout(0.4))\n",
        "\n",
        "#Output Layer\n",
        "AlexNet.add(Dense(10))\n",
        "AlexNet.add(BatchNormalization())\n",
        "AlexNet.add(Activation('softmax'))\n",
        "\n",
        "#Model Summary\n",
        "AlexNet.summary()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 8, 8, 96)          34944     \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 8, 8, 96)          384       \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 8, 8, 96)          0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 4, 4, 96)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 4, 4, 256)         614656    \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 4, 4, 256)         1024      \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 4, 4, 256)         0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 2, 2, 256)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 2, 2, 384)         885120    \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 2, 2, 384)         1536      \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 2, 2, 384)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 2, 2, 384)         1327488   \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 2, 2, 384)         1536      \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 2, 2, 384)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 2, 2, 256)         884992    \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 2, 2, 256)         1024      \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 2, 2, 256)         0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 1, 1, 256)         0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 4096)              1052672   \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 4096)              16384     \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 4096)              0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 4096)              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 4096)              16781312  \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 4096)              16384     \n",
            "_________________________________________________________________\n",
            "activation_6 (Activation)    (None, 4096)              0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 4096)              0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1000)              4097000   \n",
            "_________________________________________________________________\n",
            "batch_normalization_7 (Batch (None, 1000)              4000      \n",
            "_________________________________________________________________\n",
            "activation_7 (Activation)    (None, 1000)              0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 1000)              0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 10)                10010     \n",
            "_________________________________________________________________\n",
            "batch_normalization_8 (Batch (None, 10)                40        \n",
            "_________________________________________________________________\n",
            "activation_8 (Activation)    (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 25,730,506\n",
            "Trainable params: 25,709,350\n",
            "Non-trainable params: 21,156\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EssX_oC6Sb-o"
      },
      "source": [
        "AlexNet.compile(loss = keras.losses.categorical_crossentropy, optimizer= 'adam', metrics=['accuracy'])"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hAyVww78S_-z"
      },
      "source": [
        "### Importing CIFAR dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "45Q8ccuwS66b",
        "outputId": "6c760bd7-0f89-41ab-dcb5-9df47a9f0698"
      },
      "source": [
        "\n",
        "#Keras library for CIFAR dataset\n",
        "from keras.datasets import cifar10\n",
        "(x_train, y_train),(x_test, y_test)=cifar10.load_data()\n",
        "\n",
        "#Train-validation-test split\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train,x_val,y_train,y_val=train_test_split(x_train,y_train,test_size=.3)\n",
        "\n",
        "\n",
        "#Dimension of the CIFAR10 dataset\n",
        "print((x_train.shape,y_train.shape))\n",
        "print((x_val.shape,y_val.shape))\n",
        "print((x_test.shape,y_test.shape))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 2s 0us/step\n",
            "((35000, 32, 32, 3), (35000, 1))\n",
            "((15000, 32, 32, 3), (15000, 1))\n",
            "((10000, 32, 32, 3), (10000, 1))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "61ySv80GTEWz",
        "outputId": "caa37c7f-aff1-412c-a1d7-9578ff5c3327"
      },
      "source": [
        "#Onehot Encoding the labels.\n",
        "from sklearn.utils.multiclass import unique_labels\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "#Since we have 10 classes we should expect the shape[1] of y_train,y_val and y_test to change from 1 to 10\n",
        "y_train=to_categorical(y_train)\n",
        "y_val=to_categorical(y_val)\n",
        "y_test=to_categorical(y_test)\n",
        "\n",
        "#Verifying the dimension after one hot encoding\n",
        "print((x_train.shape,y_train.shape))\n",
        "print((x_val.shape,y_val.shape))\n",
        "print((x_test.shape,y_test.shape))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "((35000, 32, 32, 3), (35000, 10))\n",
            "((15000, 32, 32, 3), (15000, 10))\n",
            "((10000, 32, 32, 3), (10000, 10))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vKEZp5ZaTOyt",
        "outputId": "48bd1d3d-b22c-47d4-b5d7-e58b01f618db"
      },
      "source": [
        "#Defining the parameters\n",
        "batch_size= 100\n",
        "epochs=50\n",
        "#Training the model\n",
        "AlexNet.fit(x_train, y_train,batch_size=batch_size,epochs=epochs,verbose=1,validation_data=(x_val, y_val))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "350/350 [==============================] - 43s 25ms/step - loss: 1.7934 - accuracy: 0.3435 - val_loss: 2.1465 - val_accuracy: 0.2694\n",
            "Epoch 2/50\n",
            "350/350 [==============================] - 8s 23ms/step - loss: 1.3883 - accuracy: 0.5134 - val_loss: 1.6163 - val_accuracy: 0.4496\n",
            "Epoch 3/50\n",
            "350/350 [==============================] - 8s 23ms/step - loss: 1.2334 - accuracy: 0.5709 - val_loss: 2.1382 - val_accuracy: 0.3231\n",
            "Epoch 4/50\n",
            "350/350 [==============================] - 8s 23ms/step - loss: 1.1195 - accuracy: 0.6142 - val_loss: 1.6374 - val_accuracy: 0.4469\n",
            "Epoch 5/50\n",
            "350/350 [==============================] - 8s 23ms/step - loss: 1.0201 - accuracy: 0.6522 - val_loss: 1.2566 - val_accuracy: 0.5813\n",
            "Epoch 6/50\n",
            "350/350 [==============================] - 8s 23ms/step - loss: 0.9373 - accuracy: 0.6845 - val_loss: 1.3100 - val_accuracy: 0.5532\n",
            "Epoch 7/50\n",
            "350/350 [==============================] - 8s 23ms/step - loss: 0.8371 - accuracy: 0.7192 - val_loss: 1.6480 - val_accuracy: 0.4988\n",
            "Epoch 8/50\n",
            "350/350 [==============================] - 8s 23ms/step - loss: 0.7596 - accuracy: 0.7458 - val_loss: 1.7917 - val_accuracy: 0.4207\n",
            "Epoch 9/50\n",
            "350/350 [==============================] - 8s 23ms/step - loss: 0.6676 - accuracy: 0.7815 - val_loss: 1.3940 - val_accuracy: 0.5529\n",
            "Epoch 10/50\n",
            "350/350 [==============================] - 8s 23ms/step - loss: 0.5821 - accuracy: 0.8129 - val_loss: 1.7126 - val_accuracy: 0.4843\n",
            "Epoch 11/50\n",
            "350/350 [==============================] - 8s 23ms/step - loss: 0.4990 - accuracy: 0.8395 - val_loss: 1.4334 - val_accuracy: 0.5659\n",
            "Epoch 12/50\n",
            "350/350 [==============================] - 8s 23ms/step - loss: 0.4330 - accuracy: 0.8623 - val_loss: 2.0217 - val_accuracy: 0.4382\n",
            "Epoch 13/50\n",
            "350/350 [==============================] - 8s 23ms/step - loss: 0.3710 - accuracy: 0.8847 - val_loss: 1.5566 - val_accuracy: 0.5541\n",
            "Epoch 14/50\n",
            "350/350 [==============================] - 8s 23ms/step - loss: 0.3240 - accuracy: 0.8977 - val_loss: 1.6050 - val_accuracy: 0.5321\n",
            "Epoch 15/50\n",
            "350/350 [==============================] - 8s 23ms/step - loss: 0.2809 - accuracy: 0.9144 - val_loss: 2.0208 - val_accuracy: 0.4589\n",
            "Epoch 16/50\n",
            "350/350 [==============================] - 8s 23ms/step - loss: 0.2504 - accuracy: 0.9216 - val_loss: 1.5243 - val_accuracy: 0.5791\n",
            "Epoch 17/50\n",
            "350/350 [==============================] - 8s 23ms/step - loss: 0.2224 - accuracy: 0.9305 - val_loss: 1.8241 - val_accuracy: 0.5345\n",
            "Epoch 18/50\n",
            "350/350 [==============================] - 8s 23ms/step - loss: 0.1980 - accuracy: 0.9385 - val_loss: 1.6943 - val_accuracy: 0.5341\n",
            "Epoch 19/50\n",
            "350/350 [==============================] - 8s 23ms/step - loss: 0.1695 - accuracy: 0.9491 - val_loss: 2.1247 - val_accuracy: 0.4993\n",
            "Epoch 20/50\n",
            "350/350 [==============================] - 8s 23ms/step - loss: 0.1653 - accuracy: 0.9494 - val_loss: 1.6197 - val_accuracy: 0.5648\n",
            "Epoch 21/50\n",
            "350/350 [==============================] - 8s 23ms/step - loss: 0.1501 - accuracy: 0.9551 - val_loss: 1.9051 - val_accuracy: 0.5163\n",
            "Epoch 22/50\n",
            "350/350 [==============================] - 8s 23ms/step - loss: 0.1375 - accuracy: 0.9591 - val_loss: 2.1061 - val_accuracy: 0.4887\n",
            "Epoch 23/50\n",
            "350/350 [==============================] - 8s 23ms/step - loss: 0.1281 - accuracy: 0.9601 - val_loss: 1.8822 - val_accuracy: 0.5431\n",
            "Epoch 24/50\n",
            "350/350 [==============================] - 8s 23ms/step - loss: 0.1319 - accuracy: 0.9601 - val_loss: 1.5715 - val_accuracy: 0.5851\n",
            "Epoch 25/50\n",
            "350/350 [==============================] - 8s 23ms/step - loss: 0.1227 - accuracy: 0.9626 - val_loss: 1.7601 - val_accuracy: 0.5599\n",
            "Epoch 26/50\n",
            "350/350 [==============================] - 8s 23ms/step - loss: 0.1124 - accuracy: 0.9657 - val_loss: 2.0801 - val_accuracy: 0.5389\n",
            "Epoch 27/50\n",
            "350/350 [==============================] - 8s 23ms/step - loss: 0.1082 - accuracy: 0.9681 - val_loss: 3.1879 - val_accuracy: 0.3656\n",
            "Epoch 28/50\n",
            "350/350 [==============================] - 8s 23ms/step - loss: 0.1131 - accuracy: 0.9649 - val_loss: 2.4869 - val_accuracy: 0.4266\n",
            "Epoch 29/50\n",
            "350/350 [==============================] - 8s 23ms/step - loss: 0.0871 - accuracy: 0.9736 - val_loss: 2.1865 - val_accuracy: 0.5241\n",
            "Epoch 30/50\n",
            "350/350 [==============================] - 8s 23ms/step - loss: 0.0989 - accuracy: 0.9692 - val_loss: 2.1693 - val_accuracy: 0.4791\n",
            "Epoch 31/50\n",
            "350/350 [==============================] - 8s 23ms/step - loss: 0.0896 - accuracy: 0.9731 - val_loss: 2.1346 - val_accuracy: 0.5111\n",
            "Epoch 32/50\n",
            "350/350 [==============================] - 8s 23ms/step - loss: 0.0781 - accuracy: 0.9756 - val_loss: 1.9992 - val_accuracy: 0.5459\n",
            "Epoch 33/50\n",
            "350/350 [==============================] - 8s 23ms/step - loss: 0.0818 - accuracy: 0.9727 - val_loss: 2.1163 - val_accuracy: 0.5027\n",
            "Epoch 34/50\n",
            "350/350 [==============================] - 8s 23ms/step - loss: 0.0757 - accuracy: 0.9772 - val_loss: 1.7696 - val_accuracy: 0.5853\n",
            "Epoch 35/50\n",
            "350/350 [==============================] - 8s 23ms/step - loss: 0.0799 - accuracy: 0.9760 - val_loss: 1.9068 - val_accuracy: 0.5553\n",
            "Epoch 36/50\n",
            "350/350 [==============================] - 8s 23ms/step - loss: 0.0690 - accuracy: 0.9786 - val_loss: 2.0279 - val_accuracy: 0.5509\n",
            "Epoch 37/50\n",
            "350/350 [==============================] - 8s 23ms/step - loss: 0.0681 - accuracy: 0.9785 - val_loss: 1.9320 - val_accuracy: 0.5607\n",
            "Epoch 38/50\n",
            "350/350 [==============================] - 8s 23ms/step - loss: 0.0705 - accuracy: 0.9784 - val_loss: 2.1916 - val_accuracy: 0.5049\n",
            "Epoch 39/50\n",
            "350/350 [==============================] - 8s 23ms/step - loss: 0.0709 - accuracy: 0.9763 - val_loss: 1.6528 - val_accuracy: 0.6050\n",
            "Epoch 40/50\n",
            "350/350 [==============================] - 8s 23ms/step - loss: 0.0615 - accuracy: 0.9807 - val_loss: 2.3544 - val_accuracy: 0.4887\n",
            "Epoch 41/50\n",
            "350/350 [==============================] - 8s 23ms/step - loss: 0.0552 - accuracy: 0.9820 - val_loss: 3.2412 - val_accuracy: 0.3952\n",
            "Epoch 42/50\n",
            "350/350 [==============================] - 8s 23ms/step - loss: 0.0678 - accuracy: 0.9793 - val_loss: 2.7380 - val_accuracy: 0.4607\n",
            "Epoch 43/50\n",
            "350/350 [==============================] - 8s 23ms/step - loss: 0.0678 - accuracy: 0.9787 - val_loss: 2.1014 - val_accuracy: 0.5599\n",
            "Epoch 44/50\n",
            "350/350 [==============================] - 8s 23ms/step - loss: 0.0506 - accuracy: 0.9842 - val_loss: 2.2160 - val_accuracy: 0.5322\n",
            "Epoch 45/50\n",
            "350/350 [==============================] - 8s 23ms/step - loss: 0.0602 - accuracy: 0.9817 - val_loss: 2.3905 - val_accuracy: 0.5090\n",
            "Epoch 46/50\n",
            "350/350 [==============================] - 8s 23ms/step - loss: 0.0588 - accuracy: 0.9824 - val_loss: 1.8609 - val_accuracy: 0.5889\n",
            "Epoch 47/50\n",
            "350/350 [==============================] - 8s 23ms/step - loss: 0.0468 - accuracy: 0.9860 - val_loss: 2.2074 - val_accuracy: 0.5003\n",
            "Epoch 48/50\n",
            "350/350 [==============================] - 8s 23ms/step - loss: 0.0537 - accuracy: 0.9827 - val_loss: 2.7031 - val_accuracy: 0.4587\n",
            "Epoch 49/50\n",
            "350/350 [==============================] - 8s 23ms/step - loss: 0.0541 - accuracy: 0.9837 - val_loss: 2.0744 - val_accuracy: 0.5589\n",
            "Epoch 50/50\n",
            "350/350 [==============================] - 8s 23ms/step - loss: 0.0508 - accuracy: 0.9829 - val_loss: 2.0381 - val_accuracy: 0.5754\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fe5ba26b410>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l-__awIVTUV-",
        "outputId": "7d45584a-056a-4d38-9958-4c8f0256e8ce"
      },
      "source": [
        "AlexNet.evaluate(x_test,y_test)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 2s 6ms/step - loss: 2.0412 - accuracy: 0.5691\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2.0412001609802246, 0.569100022315979]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xQ-yPj24Vj-h"
      },
      "source": [
        "!mkdir -p saved_model\n",
        "AlexNet.save('saved_model/AlexNetModel.h5')"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y9XLj9f2Vsko"
      },
      "source": [
        "### Transfer Learning using the AlexNet Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "16xZ_xRMVn_O"
      },
      "source": [
        "import tensorflow as tf\n",
        "import os"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MhD5hqW1VykJ"
      },
      "source": [
        "Base_model = tf.keras.models.load_model('saved_model/AlexNetModel.h5')"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dOF0PxWGV1N6"
      },
      "source": [
        "nb_train_samples =60000\n",
        "nb_valid_samples =10000\n",
        "num_classes = 10"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RUhZLU6dWMsz",
        "outputId": "8ebbe4b4-e5e3-48d9-f8ae-bac5f8b7a1a1"
      },
      "source": [
        "\n",
        "(X_train,Y_train), (X_valid, Y_valid) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "# expand new axis, channel axis \n",
        "X_train = np.expand_dims(X_train, axis=-1)\n",
        "\n",
        "# [optional]: we may need 3 channel (instead of 1)\n",
        "X_train = np.repeat(X_train, 3, axis=-1)\n",
        "\n",
        "# it's always better to normalize \n",
        "X_train = X_train.astype('float32') / 255\n",
        "\n",
        "# resize the input shape , i.e. old shape: 28, new shape: 32\n",
        "X_train = tf.image.resize(X_train, [32,32]) # if we want to resize \n",
        "\n",
        "print(X_train.shape)\n",
        "\n",
        "X_valid = np.expand_dims(X_valid, axis=-1)\n",
        "\n",
        "# [optional]: we may need 3 channel (instead of 1)\n",
        "X_valid = np.repeat(X_valid, 3, axis=-1)\n",
        "\n",
        "# it's always better to normalize \n",
        "X_valid = X_valid.astype('float32') / 255\n",
        "\n",
        "# resize the input shape , i.e. old shape: 28, new shape: 32\n",
        "X_valid = tf.image.resize(X_valid, [32,32]) # if we want to resize \n",
        "\n",
        "print(X_valid.shape)\n",
        "\n",
        "from keras.utils import np_utils\n",
        "Y_train = np_utils.to_categorical(Y_train[:nb_train_samples], num_classes)\n",
        "Y_valid = np_utils.to_categorical(Y_valid[:nb_valid_samples], num_classes)\n",
        "\n",
        "print((X_train.shape,Y_train.shape))\n",
        "print((X_valid.shape,Y_valid.shape))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "(60000, 32, 32, 3)\n",
            "(10000, 32, 32, 3)\n",
            "(TensorShape([60000, 32, 32, 3]), (60000, 10))\n",
            "(TensorShape([10000, 32, 32, 3]), (10000, 10))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IcoPRLbKWQEH"
      },
      "source": [
        "\n",
        "from keras.models import load_model\n",
        "new_model = load_model('saved_model/AlexNetModel.h5')\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lP7Ch3bHWVHG",
        "outputId": "54251824-d6e9-4184-f785-9a8598c38ab2"
      },
      "source": [
        "new_model.summary()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 8, 8, 96)          34944     \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 8, 8, 96)          384       \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 8, 8, 96)          0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 4, 4, 96)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 4, 4, 256)         614656    \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 4, 4, 256)         1024      \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 4, 4, 256)         0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 2, 2, 256)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 2, 2, 384)         885120    \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 2, 2, 384)         1536      \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 2, 2, 384)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 2, 2, 384)         1327488   \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 2, 2, 384)         1536      \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 2, 2, 384)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 2, 2, 256)         884992    \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 2, 2, 256)         1024      \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 2, 2, 256)         0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 1, 1, 256)         0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 4096)              1052672   \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 4096)              16384     \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 4096)              0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 4096)              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 4096)              16781312  \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 4096)              16384     \n",
            "_________________________________________________________________\n",
            "activation_6 (Activation)    (None, 4096)              0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 4096)              0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1000)              4097000   \n",
            "_________________________________________________________________\n",
            "batch_normalization_7 (Batch (None, 1000)              4000      \n",
            "_________________________________________________________________\n",
            "activation_7 (Activation)    (None, 1000)              0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 1000)              0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 10)                10010     \n",
            "_________________________________________________________________\n",
            "batch_normalization_8 (Batch (None, 10)                40        \n",
            "_________________________________________________________________\n",
            "activation_8 (Activation)    (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 25,730,506\n",
            "Trainable params: 25,709,350\n",
            "Non-trainable params: 21,156\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GP6K5zQfWXQP",
        "outputId": "02d7292d-747d-40ce-8c19-63a34f2209ce"
      },
      "source": [
        "new_model.trainable=False\n",
        "model = tf.keras.Sequential([\n",
        "    new_model,\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss = 'categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "sequential (Sequential)      (None, 10)                25730506  \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 10)                110       \n",
            "=================================================================\n",
            "Total params: 25,730,616\n",
            "Trainable params: 110\n",
            "Non-trainable params: 25,730,506\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9bwQqEG4Wbyz",
        "outputId": "bf77a217-69e9-4174-f52c-b7f7732dbbc2"
      },
      "source": [
        "history = model.fit(X_train, Y_train, batch_size=batch_size, epochs=50, verbose=1,validation_data=(X_valid, Y_valid))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "600/600 [==============================] - 6s 9ms/step - loss: 2.3221 - accuracy: 0.0958 - val_loss: 2.3014 - val_accuracy: 0.1135\n",
            "Epoch 2/50\n",
            "600/600 [==============================] - 5s 9ms/step - loss: 2.3017 - accuracy: 0.1116 - val_loss: 2.3012 - val_accuracy: 0.1135\n",
            "Epoch 3/50\n",
            "600/600 [==============================] - 5s 9ms/step - loss: 2.3014 - accuracy: 0.1112 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
            "Epoch 4/50\n",
            "600/600 [==============================] - 5s 9ms/step - loss: 2.3013 - accuracy: 0.1122 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
            "Epoch 5/50\n",
            "600/600 [==============================] - 5s 9ms/step - loss: 2.3011 - accuracy: 0.1116 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
            "Epoch 6/50\n",
            "600/600 [==============================] - 5s 9ms/step - loss: 2.3010 - accuracy: 0.1125 - val_loss: 2.3012 - val_accuracy: 0.1135\n",
            "Epoch 7/50\n",
            "600/600 [==============================] - 5s 9ms/step - loss: 2.3013 - accuracy: 0.1128 - val_loss: 2.3012 - val_accuracy: 0.1135\n",
            "Epoch 8/50\n",
            "600/600 [==============================] - 5s 9ms/step - loss: 2.3015 - accuracy: 0.1122 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
            "Epoch 9/50\n",
            "600/600 [==============================] - 5s 9ms/step - loss: 2.3011 - accuracy: 0.1124 - val_loss: 2.3012 - val_accuracy: 0.1135\n",
            "Epoch 10/50\n",
            "600/600 [==============================] - 5s 9ms/step - loss: 2.3015 - accuracy: 0.1123 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
            "Epoch 11/50\n",
            "600/600 [==============================] - 5s 9ms/step - loss: 2.3014 - accuracy: 0.1106 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
            "Epoch 12/50\n",
            "600/600 [==============================] - 5s 9ms/step - loss: 2.3012 - accuracy: 0.1138 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
            "Epoch 13/50\n",
            "600/600 [==============================] - 5s 9ms/step - loss: 2.3015 - accuracy: 0.1112 - val_loss: 2.3012 - val_accuracy: 0.1135\n",
            "Epoch 14/50\n",
            "600/600 [==============================] - 5s 9ms/step - loss: 2.3013 - accuracy: 0.1131 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
            "Epoch 15/50\n",
            "600/600 [==============================] - 5s 9ms/step - loss: 2.3012 - accuracy: 0.1148 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
            "Epoch 16/50\n",
            "600/600 [==============================] - 5s 9ms/step - loss: 2.3011 - accuracy: 0.1138 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
            "Epoch 17/50\n",
            "600/600 [==============================] - 5s 9ms/step - loss: 2.3013 - accuracy: 0.1117 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
            "Epoch 18/50\n",
            "600/600 [==============================] - 5s 9ms/step - loss: 2.3014 - accuracy: 0.1114 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
            "Epoch 19/50\n",
            "600/600 [==============================] - 5s 9ms/step - loss: 2.3014 - accuracy: 0.1136 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
            "Epoch 20/50\n",
            "600/600 [==============================] - 5s 9ms/step - loss: 2.3014 - accuracy: 0.1125 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
            "Epoch 21/50\n",
            "600/600 [==============================] - 5s 9ms/step - loss: 2.3013 - accuracy: 0.1127 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
            "Epoch 22/50\n",
            "600/600 [==============================] - 5s 9ms/step - loss: 2.3011 - accuracy: 0.1125 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
            "Epoch 23/50\n",
            "600/600 [==============================] - 5s 9ms/step - loss: 2.3014 - accuracy: 0.1111 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
            "Epoch 24/50\n",
            "600/600 [==============================] - 5s 9ms/step - loss: 2.3015 - accuracy: 0.1114 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
            "Epoch 25/50\n",
            "600/600 [==============================] - 5s 9ms/step - loss: 2.3012 - accuracy: 0.1141 - val_loss: 2.3014 - val_accuracy: 0.1135\n",
            "Epoch 26/50\n",
            "600/600 [==============================] - 5s 9ms/step - loss: 2.3011 - accuracy: 0.1129 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
            "Epoch 27/50\n",
            "600/600 [==============================] - 5s 9ms/step - loss: 2.3013 - accuracy: 0.1118 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
            "Epoch 28/50\n",
            "600/600 [==============================] - 5s 9ms/step - loss: 2.3010 - accuracy: 0.1136 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
            "Epoch 29/50\n",
            "600/600 [==============================] - 5s 9ms/step - loss: 2.3011 - accuracy: 0.1117 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
            "Epoch 30/50\n",
            "600/600 [==============================] - 5s 9ms/step - loss: 2.3015 - accuracy: 0.1109 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
            "Epoch 31/50\n",
            "600/600 [==============================] - 5s 9ms/step - loss: 2.3011 - accuracy: 0.1136 - val_loss: 2.3012 - val_accuracy: 0.1135\n",
            "Epoch 32/50\n",
            "600/600 [==============================] - 5s 9ms/step - loss: 2.3014 - accuracy: 0.1112 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
            "Epoch 33/50\n",
            "600/600 [==============================] - 5s 9ms/step - loss: 2.3011 - accuracy: 0.1128 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
            "Epoch 34/50\n",
            "600/600 [==============================] - 5s 9ms/step - loss: 2.3017 - accuracy: 0.1117 - val_loss: 2.3014 - val_accuracy: 0.1135\n",
            "Epoch 35/50\n",
            "600/600 [==============================] - 5s 9ms/step - loss: 2.3014 - accuracy: 0.1115 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
            "Epoch 36/50\n",
            "600/600 [==============================] - 5s 9ms/step - loss: 2.3017 - accuracy: 0.1103 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
            "Epoch 37/50\n",
            "600/600 [==============================] - 5s 9ms/step - loss: 2.3013 - accuracy: 0.1138 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
            "Epoch 38/50\n",
            "600/600 [==============================] - 5s 9ms/step - loss: 2.3011 - accuracy: 0.1100 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
            "Epoch 39/50\n",
            "600/600 [==============================] - 5s 9ms/step - loss: 2.3012 - accuracy: 0.1123 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
            "Epoch 40/50\n",
            "600/600 [==============================] - 5s 9ms/step - loss: 2.3012 - accuracy: 0.1135 - val_loss: 2.3013 - val_accuracy: 0.1135\n",
            "Epoch 41/50\n",
            "600/600 [==============================] - 5s 9ms/step - loss: 2.3012 - accuracy: 0.1126 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
            "Epoch 42/50\n",
            "600/600 [==============================] - 5s 9ms/step - loss: 2.3012 - accuracy: 0.1123 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
            "Epoch 43/50\n",
            "600/600 [==============================] - 5s 9ms/step - loss: 2.3015 - accuracy: 0.1122 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
            "Epoch 44/50\n",
            "600/600 [==============================] - 5s 9ms/step - loss: 2.3016 - accuracy: 0.1110 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
            "Epoch 45/50\n",
            "600/600 [==============================] - 5s 9ms/step - loss: 2.3014 - accuracy: 0.1121 - val_loss: 2.3009 - val_accuracy: 0.1135\n",
            "Epoch 46/50\n",
            "600/600 [==============================] - 5s 9ms/step - loss: 2.3015 - accuracy: 0.1111 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
            "Epoch 47/50\n",
            "600/600 [==============================] - 5s 9ms/step - loss: 2.3015 - accuracy: 0.1118 - val_loss: 2.3012 - val_accuracy: 0.1135\n",
            "Epoch 48/50\n",
            "600/600 [==============================] - 5s 9ms/step - loss: 2.3014 - accuracy: 0.1134 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
            "Epoch 49/50\n",
            "600/600 [==============================] - 5s 9ms/step - loss: 2.3012 - accuracy: 0.1121 - val_loss: 2.3012 - val_accuracy: 0.1135\n",
            "Epoch 50/50\n",
            "600/600 [==============================] - 5s 9ms/step - loss: 2.3016 - accuracy: 0.1126 - val_loss: 2.3010 - val_accuracy: 0.1135\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ziW4jERYWlxU"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}